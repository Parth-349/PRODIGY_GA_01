{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "authorship_tag": "ABX9TyN2AN3l16TUquzFzOhrRFGT",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Parth-349/PRODIGY_GA_01/blob/main/gpt2_finetuning_poetic_ipynb.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xrx1023bmioZ"
      },
      "outputs": [],
      "source": [
        "# Step 1: Install Required Libraries\n",
        "!pip install transformers datasets accelerate"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "c946b859"
      },
      "source": [
        "# Step 2: Import Libraries\n",
        "from datasets import load_dataset, Dataset\n",
        "from transformers import GPT2Tokenizer, GPT2LMHeadModel, DataCollatorForLanguageModeling\n",
        "from transformers import Trainer, TrainingArguments, pipeline"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4b5534d6"
      },
      "source": [
        "# Step 3: Load Pretrained GPT-2 Tokenizer and Model\n",
        "tokenizer = GPT2Tokenizer.from_pretrained(\"gpt2\")\n",
        "tokenizer.pad_token = tokenizer.eos_token\n",
        "model = GPT2LMHeadModel.from_pretrained(\"gpt2\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8af49e84"
      },
      "source": [
        "# Step 4: Define Custom Dataset (using the provided text)\n",
        "text_data = \"\"\"The knight crossed the valley with fire behind him.\n",
        "The moonlight lit up her journal as she wrote her last words.\n",
        "Beneath the waves, ancient ruins whispered tales of betrayal.\n",
        "Every shadow told a story; every whisper carried a warning.\n",
        "Let go, for even the river forgets its beginning.\n",
        "Sometimes the questions matter more than the answers.\n",
        "You are not broken; you are just unfolding.\n",
        "Grief is just love without a place to land.\n",
        "We are all stories pretending to be solid.\n",
        "Truth waits in the quiet corners where ego dares not look.\n",
        "\"\"\"\n",
        "\n",
        "text_lines = text_data.strip().split('\\n')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Step 5: Load and Tokenize Dataset\n",
        "# Create a dataset from the loaded text\n",
        "dataset = Dataset.from_dict({\"text\": text_lines})\n",
        "\n",
        "\n",
        "def tokenize_function(examples):\n",
        "    return tokenizer(examples[\"text\"], padding=\"max_length\", truncation=True, max_length=512)\n",
        "\n",
        "tokenized_dataset = dataset.map(tokenize_function, batched=True)"
      ],
      "metadata": {
        "id": "Wf08iV97JqBi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8016bcc5"
      },
      "source": [
        "# Step 6: Prepare Training Components\n",
        "data_collator = DataCollatorForLanguageModeling(\n",
        "    tokenizer=tokenizer, mlm=False,\n",
        ")\n",
        "\n",
        "training_args = TrainingArguments(\n",
        "    output_dir=\"./gpt2-finetuned\",\n",
        "    overwrite_output_dir=True,\n",
        "    num_train_epochs=5,\n",
        "    per_device_train_batch_size=2,\n",
        "    save_steps=500,\n",
        "    save_total_limit=1,\n",
        "    logging_steps=100,\n",
        "    prediction_loss_only=True,\n",
        "    fp16=True,\n",
        "    report_to=\"none\",\n",
        ")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "622a829b"
      },
      "source": [
        "# Step 7: Train the Model\n",
        "trainer = Trainer(\n",
        "    model=model,\n",
        "    args=training_args,\n",
        "    train_dataset=tokenized_dataset,\n",
        "    tokenizer=tokenizer,\n",
        "    data_collator=data_collator,\n",
        ")\n",
        "\n",
        "trainer.train()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "a0941a04"
      },
      "source": [
        "# Step 8: Save Model\n",
        "model.save_pretrained(\"gpt2-finetuned\")\n",
        "tokenizer.save_pretrained(\"gpt2-finetuned\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "e40ecf1e"
      },
      "source": [
        "# Step 9: Generate Text from the Fine-Tuned Model\n",
        "generator = pipeline(\"text-generation\", model=\"gpt2-finetuned\", tokenizer=tokenizer)\n",
        "prompt = \"Once upon a time\"\n",
        "output = generator(\n",
        "    prompt,\n",
        "    max_new_tokens=100,\n",
        "    num_return_sequences=1,\n",
        "    do_sample=True,\n",
        "    temperature=0.9,\n",
        "    top_p=0.95,\n",
        "    top_k=50,\n",
        "    repetition_penalty=1.2\n",
        ")\n",
        "print(output[0]['generated_text'])"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}
