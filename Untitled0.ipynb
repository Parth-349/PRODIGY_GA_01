{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "authorship_tag": "ABX9TyM7KKIzosf4zSsU2ANUi0pL",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Parth-349/PRODIGY_GA_01/blob/main/Untitled0.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xrx1023bmioZ"
      },
      "outputs": [],
      "source": [
        "# Step 1: Install Required Libraries\n",
        "!pip install transformers datasets accelerate"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "c946b859"
      },
      "source": [
        "# Step 2: Import Libraries\n",
        "from datasets import load_dataset, Dataset\n",
        "from transformers import GPT2Tokenizer, GPT2LMHeadModel, DataCollatorForLanguageModeling\n",
        "from transformers import Trainer, TrainingArguments\n",
        "from transformers import pipeline"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4b5534d6"
      },
      "source": [
        "# Step 3: Load Pretrained GPT-2 Tokenizer and Model\n",
        "tokenizer = GPT2Tokenizer.from_pretrained(\"gpt2\")\n",
        "tokenizer.pad_token = tokenizer.eos_token\n",
        "model = GPT2LMHeadModel.from_pretrained(\"gpt2\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8af49e84"
      },
      "source": [
        "# Step 4: Define Custom Dataset (using the provided text)\n",
        "text_data = \"\"\"The knight crossed the valley with fire behind him.\n",
        "The moonlight lit up her journal as she wrote her last words.\n",
        "Beneath the waves, ancient ruins whispered tales of betrayal.\n",
        "Every shadow told a story; every whisper carried a warning.\n",
        "...\n",
        "\"\"\"\n",
        "\n",
        "# Split the text into lines to simulate the previous dataset structure\n",
        "text_lines = text_data.strip().split('\\n')\n",
        "\n",
        "# Step 5: Load and Tokenize Dataset\n",
        "# Create a dataset from the loaded text\n",
        "dataset = Dataset.from_dict({\"text\": text_lines})\n",
        "\n",
        "\n",
        "def tokenize_function(examples):\n",
        "    return tokenizer(examples[\"text\"], padding=\"max_length\", truncation=True, max_length=512)\n",
        "\n",
        "tokenized_dataset = dataset.map(tokenize_function, batched=True)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8016bcc5"
      },
      "source": [
        "# Step 6: Prepare Training Components\n",
        "data_collator = DataCollatorForLanguageModeling(\n",
        "    tokenizer=tokenizer, mlm=False,\n",
        ")\n",
        "\n",
        "training_args = TrainingArguments(\n",
        "    output_dir=\"./gpt2-finetuned\",\n",
        "    overwrite_output_dir=True,\n",
        "    num_train_epochs=5,\n",
        "    per_device_train_batch_size=2,\n",
        "    save_steps=500,\n",
        "    save_total_limit=1,\n",
        "    logging_steps=100,\n",
        "    prediction_loss_only=True,\n",
        "    fp16=True,\n",
        "    report_to=\"none\",\n",
        ")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "622a829b"
      },
      "source": [
        "# Step 7: Train the Model\n",
        "trainer = Trainer(\n",
        "    model=model,\n",
        "    args=training_args,\n",
        "    train_dataset=tokenized_dataset,\n",
        "    tokenizer=tokenizer,\n",
        "    data_collator=data_collator,\n",
        ")\n",
        "\n",
        "trainer.train()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "a0941a04"
      },
      "source": [
        "# Step 8: Save Model\n",
        "model.save_pretrained(\"gpt2-finetuned\")\n",
        "tokenizer.save_pretrained(\"gpt2-finetuned\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "e40ecf1e"
      },
      "source": [
        "# Step 9: Generate Text from the Fine-Tuned Model\n",
        "generator = pipeline(\"text-generation\", model=\"gpt2-finetuned\", tokenizer=tokenizer)\n",
        "prompt = \"Once upon a time\"\n",
        "output = generator(prompt, max_new_tokens=100, num_return_sequences=1, repetition_penalty=1.2)\n",
        "print(output[0]['generated_text'])"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}