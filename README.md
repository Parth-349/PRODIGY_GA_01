# PRODIGY_GA_01_Text Generation with GPT-2
Training a model to generate coherent and contextually relevant text based on a given prompt. Starting with GPT-2, a transformer model developed by OpenAI.

## What This Notebook Does:
 
-Installs required libraries: transformers, datasets, and accelerate

-Loads the pretrained gpt2 model and tokenizer

-Defines a custom poetic/philosophical dataset

-Tokenizes the text using Hugging Face’s tokenizer

-Fine-tunes GPT-2 using Trainer API for several epochs

-Generates creative text from a prompt using the updated model


## Libraries Used

 transformers — for model loading, tokenization, training, and text generation

 datasets — for handling and tokenizing the custom training data

 accelerate — to enable efficient training (especially with GPUs)

 torch — used under the hood by the Hugging Face Trainer for model training

## Sample Output
Prompt: "Once upon a time"

## Generated:  
Once upon a time in the darkest corner of their hearts, they knew that God was with them. And not even Jesus himself could speak to him honestly.[1]
We can never know who died for us all if we are ever told what happened after our deaths as one man's testimony begins anew and fades away when another hears it again from its source: death itself.[2][3]] Acknowledge your own mortality by forgiving yourself deeply but knowing how much you deserve whatever pain awaits on an island somewhere inside yours
 
---

##  View Output.ipynb Without GitHub Errors

If GitHub fails to render the Output notebook properly, you can still view it using [NBViewer](https://nbviewer.org):


➡️ [View Output.ipynb on NBViewer](https://nbviewer.org/github/Parth-349/PRODIGY_GA_01/blob/main/Output.ipynb)

---

## Author

Parth Ubhad

GitHub: [Parth-349](https://github.com/Parth-349)






