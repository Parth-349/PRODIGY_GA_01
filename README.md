# PRODIGY_GA_01
Training a model to generate coherent and contextually relevant text based on a given prompt. Starting with GPT-2, a transformer model developed by OpenAI.

ğŸš€ What This Notebook Does
-Installs required libraries: transformers, datasets, and accelerate

-Loads the pretrained gpt2 model and tokenizer

-Defines a custom poetic/philosophical dataset

-Tokenizes the text using Hugging Faceâ€™s tokenizer

-Fine-tunes GPT-2 using Trainer API for several epochs

-Generates creative text from a prompt using the updated model


ğŸ› ï¸ Libraries Used

ğŸ¤— transformers â€” for model loading, tokenization, training, and text generation

ğŸ“š datasets â€” for handling and tokenizing the custom training data

âš¡ accelerate â€” to enable efficient training (especially with GPUs)

ğŸ”¥ torch â€” used under the hood by the Hugging Face Trainer for model training

 Sample Output
Prompt: "Once upon a time"

Generated:  Once upon a time in the darkest corner of their hearts, they knew that God was with them. And not even Jesus himself could speak to him honestly.[1]
We can never know who died for us all if we are ever told what happened after our deaths as one man's testimony begins anew and fades away when another hears it again from its source: death itself.[2][3]] Acknowledge your own mortality by forgiving yourself deeply but knowing how much you deserve whatever pain awaits on an island somewhere inside yours
 
