# PRODIGY_GA_01
Training a model to generate coherent and contextually relevant text based on a given prompt. Starting with GPT-2, a transformer model developed by OpenAI.

ğŸš€ What This Notebook Does
-Installs required libraries: transformers, datasets, and accelerate

-Loads the pretrained gpt2 model and tokenizer

-Defines a custom poetic/philosophical dataset

-Tokenizes the text using Hugging Faceâ€™s tokenizer

-Fine-tunes GPT-2 using Trainer API for several epochs

-Generates creative text from a prompt using the updated model


ğŸ› ï¸ Libraries Used

ğŸ¤— transformers â€” for model loading, tokenization, training, and text generation

ğŸ“š datasets â€” for handling and tokenizing the custom training data

âš¡ accelerate â€” to enable efficient training (especially with GPUs)

ğŸ”¥ torch â€” used under the hood by the Hugging Face Trainer for model training

 Sample Output
Prompt: "Once upon a time"

Generated:  Once upon a time the wind blew through all corners of space like its own sails. For there was still hope after much suffering, but it seemed that even death itself could not keep quiet for long. It would always have to hear his voice as if in warning order: never again did she sing songs with an easy vow â€“ ever before her heart gave up singing from within and began once more listening only above themâ€¦
That's when I heard my father say "I'll tell you what will befall us." 
